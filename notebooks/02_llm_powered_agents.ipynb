{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating LLM-Powered A2A Agents\n",
    "\n",
    "This notebook demonstrates how to create and use agents powered by large language models (LLMs) like OpenAI's GPT and Anthropic's Claude with the Python A2A package. We'll explore:\n",
    "\n",
    "1. Setting up LLM-powered agents\n",
    "2. Customizing agent behavior with system prompts\n",
    "3. Comparing responses from different LLM providers\n",
    "4. Implementing function calling capabilities\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's make sure we have the Python A2A package installed with the necessary dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"python-a2a[openai,anthropic]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import the necessary components from the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from python_a2a import (\n",
    "    Message, TextContent, FunctionCallContent, FunctionResponseContent, FunctionParameter,\n",
    "    MessageRole, Conversation,\n",
    "    OpenAIA2AServer, AnthropicA2AServer,\n",
    "    pretty_print_message, pretty_print_conversation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up API Keys\n",
    "\n",
    "To use the LLM-powered agents, we need API keys for the respective providers. You should set these as environment variables or enter them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API keys here (or use environment variables)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-api-key\"\n",
    "\n",
    "# Get API keys from environment\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Check if API keys are available\n",
    "if not openai_api_key:\n",
    "    print(\"‚ö†Ô∏è OpenAI API key not found. OpenAI examples will not work.\")\n",
    "else:\n",
    "    print(\"‚úÖ OpenAI API key found.\")\n",
    "    \n",
    "if not anthropic_api_key:\n",
    "    print(\"‚ö†Ô∏è Anthropic API key not found. Claude examples will not work.\")\n",
    "else:\n",
    "    print(\"‚úÖ Anthropic API key found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating LLM-Powered Agents\n",
    "\n",
    "### OpenAI-Powered Agent\n",
    "\n",
    "Let's start by creating an agent powered by OpenAI's models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this cell if OpenAI API key is not available\n",
    "if openai_api_key:\n",
    "    # Create an OpenAI-powered agent\n",
    "    openai_agent = OpenAIA2AServer(\n",
    "        api_key=openai_api_key,\n",
    "        model=\"gpt-4\",  # You can use \"gpt-3.5-turbo\" for faster, cheaper responses\n",
    "        temperature=0.7,\n",
    "        system_prompt=\"You are a helpful assistant that provides clear, concise answers.\"\n",
    "    )\n",
    "    print(\"OpenAI agent created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic-Powered Agent\n",
    "\n",
    "Now, let's create an agent powered by Anthropic's Claude models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this cell if Anthropic API key is not available\n",
    "if anthropic_api_key:\n",
    "    # Create an Anthropic-powered agent\n",
    "    claude_agent = AnthropicA2AServer(\n",
    "        api_key=anthropic_api_key,\n",
    "        model=\"claude-3-opus-20240229\",  # You can use other Claude models too\n",
    "        temperature=0.7,\n",
    "        max_tokens=1000,\n",
    "        system_prompt=\"You are a helpful assistant that provides clear, concise answers.\"\n",
    "    )\n",
    "    print(\"Claude agent created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Agents\n",
    "\n",
    "Let's create a function to test our agents with different prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent, prompt, agent_name=\"Agent\"):\n",
    "    \"\"\"Test an agent with a prompt and print the response.\"\"\"\n",
    "    print(f\"üîç Testing {agent_name}...\")\n",
    "    \n",
    "    # Create a message with the prompt\n",
    "    message = Message(\n",
    "        content=TextContent(text=prompt),\n",
    "        role=MessageRole.USER\n",
    "    )\n",
    "    \n",
    "    # Get a response from the agent\n",
    "    response = agent.handle_message(message)\n",
    "    \n",
    "    print(f\"\\nü§ñ {agent_name} response:\\n\")\n",
    "    print(response.content.text)\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test our agents with a few different prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OpenAI agent\n",
    "if openai_api_key:\n",
    "    openai_response = test_agent(\n",
    "        openai_agent, \n",
    "        \"Explain the concept of agent interoperability and why it's important for AI systems.\",\n",
    "        \"OpenAI GPT-4\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Claude agent\n",
    "if anthropic_api_key:\n",
    "    claude_response = test_agent(\n",
    "        claude_agent, \n",
    "        \"Explain the concept of agent interoperability and why it's important for AI systems.\",\n",
    "        \"Anthropic Claude\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing Agent Behavior with System Prompts\n",
    "\n",
    "The behavior of LLM-powered agents can be customized using system prompts. Let's create specialized agents for different purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create specialized OpenAI agents (if API key is available)\n",
    "if openai_api_key:\n",
    "    # Technical documentation agent\n",
    "    technical_agent = OpenAIA2AServer(\n",
    "        api_key=openai_api_key,\n",
    "        model=\"gpt-4\",\n",
    "        temperature=0.2,  # Lower temperature for more deterministic responses\n",
    "        system_prompt=(\n",
    "            \"You are a technical documentation specialist. \"\n",
    "            \"Provide detailed, accurate explanations of technical concepts with examples where appropriate. \"\n",
    "            \"Use markdown formatting for clarity and structure.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Creative writing agent\n",
    "    creative_agent = OpenAIA2AServer(\n",
    "        api_key=openai_api_key,\n",
    "        model=\"gpt-4\",\n",
    "        temperature=0.9,  # Higher temperature for more creative responses\n",
    "        system_prompt=(\n",
    "            \"You are a creative writer and storyteller. \"\n",
    "            \"Generate engaging, imaginative content with rich descriptions and compelling narratives. \"\n",
    "            \"Be original and think outside the box.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(\"Specialized OpenAI agents created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare the responses from our specialized agents on the same prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test specialized agents (if API key is available)\n",
    "if openai_api_key:\n",
    "    prompt = \"Write about artificial intelligence agents working together.\"\n",
    "    \n",
    "    technical_response = test_agent(technical_agent, prompt, \"Technical Documentation Agent\")\n",
    "    creative_response = test_agent(creative_agent, prompt, \"Creative Writing Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Function Calling Capabilities\n",
    "\n",
    "One powerful feature of OpenAI's models is function calling, which allows the model to request the execution of specific functions. Let's implement a weather function for our OpenAI agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a weather function\n",
    "def get_weather(location, unit=\"celsius\"):\n",
    "    \"\"\"Simulate getting weather data for a location.\"\"\"\n",
    "    # In a real application, this would call a weather API\n",
    "    # For this example, we'll use mock data\n",
    "    weather_data = {\n",
    "        \"New York\": {\"temperature\": 22, \"conditions\": \"Partly Cloudy\", \"humidity\": 65, \"wind_speed\": 8},\n",
    "        \"London\": {\"temperature\": 18, \"conditions\": \"Rainy\", \"humidity\": 80, \"wind_speed\": 12},\n",
    "        \"Tokyo\": {\"temperature\": 26, \"conditions\": \"Sunny\", \"humidity\": 70, \"wind_speed\": 5},\n",
    "        \"Sydney\": {\"temperature\": 28, \"conditions\": \"Clear\", \"humidity\": 55, \"wind_speed\": 10},\n",
    "        \"Paris\": {\"temperature\": 20, \"conditions\": \"Cloudy\", \"humidity\": 60, \"wind_speed\": 7}\n",
    "    }\n",
    "    \n",
    "    # Get the weather data for the location (default to New York if not found)\n",
    "    location_data = weather_data.get(location, weather_data[\"New York\"])\n",
    "    \n",
    "    # Convert temperature if necessary\n",
    "    if unit.lower() == \"fahrenheit\":\n",
    "        location_data[\"temperature\"] = location_data[\"temperature\"] * 9/5 + 32\n",
    "        location_data[\"unit\"] = \"fahrenheit\"\n",
    "    else:\n",
    "        location_data[\"unit\"] = \"celsius\"\n",
    "    \n",
    "    # Add the location to the data\n",
    "    location_data[\"location\"] = location\n",
    "    \n",
    "    return location_data\n",
    "\n",
    "# Define the function schema for OpenAI\n",
    "weather_function = {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get the current weather for a location\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city name, e.g. New York\"\n",
    "            },\n",
    "            \"unit\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                \"description\": \"The temperature unit\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a function-enabled OpenAI agent (if API key is available)\n",
    "if openai_api_key:\n",
    "    function_agent = OpenAIA2AServer(\n",
    "        api_key=openai_api_key,\n",
    "        model=\"gpt-4\",\n",
    "        temperature=0.7,\n",
    "        system_prompt=\"You are a helpful assistant that can provide weather information.\",\n",
    "        functions=[weather_function]\n",
    "    )\n",
    "    print(\"Function-enabled OpenAI agent created successfully!\")\n",
    "\n",
    "# Now let's create a function to test the function calling capability\n",
    "def test_function_calling(agent, prompt, agent_name=\"Agent\"):\n",
    "    \"\"\"Test an agent's function calling capability with a prompt.\"\"\"\n",
    "    print(f\"üîç Testing {agent_name} function calling...\")\n",
    "    \n",
    "    # Create a message with the prompt\n",
    "    message = Message(\n",
    "        content=TextContent(text=prompt),\n",
    "        role=MessageRole.USER\n",
    "    )\n",
    "    \n",
    "    # Get a response from the agent\n",
    "    response = agent.handle_message(message)\n",
    "    \n",
    "    # Check if the response is a function call\n",
    "    if response.content.type == \"function_call\":\n",
    "        print(f\"\\nü§ñ {agent_name} wants to call a function:\\n\")\n",
    "        print(f\"Function: {response.content.name}\")\n",
    "        print(\"Parameters:\")\n",
    "        for param in response.content.parameters:\n",
    "            print(f\"  {param.name}: {param.value}\")\n",
    "        \n",
    "        # Execute the function\n",
    "        if response.content.name == \"get_weather\":\n",
    "            params = {p.name: p.value for p in response.content.parameters}\n",
    "            result = get_weather(**params)\n",
    "            \n",
    "            # Create a function response\n",
    "            function_response = Message(\n",
    "                content=FunctionResponseContent(\n",
    "                    name=\"get_weather\",\n",
    "                    response=result\n",
    "                ),\n",
    "                role=MessageRole.AGENT,\n",
    "                parent_message_id=response.message_id,\n",
    "                conversation_id=response.conversation_id\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n‚öôÔ∏è Function result:\\n\")\n",
    "            print(result)\n",
    "            \n",
    "            # Get the final response from the agent\n",
    "            final_response = agent.handle_message(function_response)\n",
    "            \n",
    "            print(f\"\\nü§ñ {agent_name} final response:\\n\")\n",
    "            print(final_response.content.text)\n",
    "            \n",
    "            return final_response\n",
    "    else:\n",
    "        print(f\"\\nü§ñ {agent_name} response (no function call):\\n\")\n",
    "        print(response.content.text)\n",
    "        \n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the function calling capability (if API key is available)\n",
    "if openai_api_key:\n",
    "    # Test with a prompt that should trigger function calling\n",
    "    function_response = test_function_calling(\n",
    "        function_agent,\n",
    "        \"What's the weather like in Tokyo right now?\",\n",
    "        \"Function-Enabled Agent\"\n",
    "    )\n",
    "    \n",
    "    # Test with another location\n",
    "    function_response2 = test_function_calling(\n",
    "        function_agent,\n",
    "        \"I'm planning a trip to London. What's the weather there?\",\n",
    "        \"Function-Enabled Agent\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Conversations\n",
    "\n",
    "LLM-powered agents can also handle full conversations with message history. Let's create a function to test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conversation(agent, prompts, agent_name=\"Agent\"):\n",
    "    \"\"\"Test an agent with a series of prompts in a conversation.\"\"\"\n",
    "    print(f\"üîç Testing {agent_name} conversation...\")\n",
    "    \n",
    "    # Create a new conversation\n",
    "    conversation = Conversation()\n",
    "    \n",
    "    # Add each prompt to the conversation and get responses\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"\\nüìù User message {i+1}: {prompt}\")\n",
    "        \n",
    "        # Add the user message to the conversation\n",
    "        conversation.create_text_message(\n",
    "            text=prompt,\n",
    "            role=MessageRole.USER\n",
    "        )\n",
    "        \n",
    "        # Send the conversation to the agent\n",
    "        updated_conversation = agent.handle_conversation(conversation)\n",
    "        \n",
    "        # Update our conversation reference\n",
    "        conversation = updated_conversation\n",
    "        \n",
    "        # Print the agent's response\n",
    "        response = conversation.messages[-1]\n",
    "        print(f\"\\nü§ñ {agent_name} response {i+1}:\\n\")\n",
    "        print(response.content.text)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test the conversation handling capability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conversation handling (if API key is available)\n",
    "if openai_api_key:\n",
    "    # Series of prompts for a conversation\n",
    "    prompts = [\n",
    "        \"What is the A2A protocol?\",\n",
    "        \"How does it relate to other agent frameworks?\",\n",
    "        \"Can you give an example of how it might be used in a real application?\"\n",
    "    ]\n",
    "    \n",
    "    conversation = test_conversation(openai_agent, prompts, \"OpenAI Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the Claude agent as well (if available):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Claude agent conversation (if API key is available)\n",
    "if anthropic_api_key:\n",
    "    # Series of prompts for a conversation\n",
    "    prompts = [\n",
    "        \"What is the A2A protocol?\",\n",
    "        \"How does it relate to other agent frameworks?\",\n",
    "        \"Can you give an example of how it might be used in a real application?\"\n",
    "    ]\n",
    "    \n",
    "    claude_conversation = test_conversation(claude_agent, prompts, \"Claude Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've learned how to:\n",
    "\n",
    "1. Create LLM-powered A2A agents using OpenAI and Anthropic models\n",
    "2. Customize agent behavior using system prompts\n",
    "3. Implement function calling capabilities\n",
    "4. Handle multi-turn conversations\n",
    "\n",
    "The Python A2A package makes it easy to create powerful, interoperable agents that can leverage the capabilities of different LLM providers. By standardizing the message format and providing a consistent interface, A2A enables seamless communication between agents regardless of the underlying model or implementation.\n",
    "\n",
    "This interoperability is key to building modular AI systems where specialized agents can collaborate to solve complex problems. Whether you're using OpenAI's GPT models, Anthropic's Claude, or other LLMs, the A2A protocol provides a common language for agent communication."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}